
\section{Methodology}
In this section, we will provide a detailed explanation on not only the modeling of road correlation, i.e. the derivation of road correlation matrix $C$, but also how to integrate it into traffic state prediction.

\subsection{Trajectory-based Road Correlation}
As mentioned in section 1, traffic states in real world varies over time. The traffic states in peak hours will even be completely different from other time intervals. However, most GCN models view the road network as a changeless graph, thus, they use adjacency matrix $A$ to represent it. On the contrary, trajectories indicate the route of vehicles, i.e. the real choices made by driver, which will give us the concrete transfer processes among roads. By leveraging them, we target to extract the road correspondence information inside transition and use a number to represent it. A simple way is regarding it as a first-order Markov process\cite{AAAI21}, and calculate the Markov transition probability as road correlation value by iterating on all trajectories. But Markov process is short in modeling high-order transition, since the next step is only related to the previous one, by its definition. Therefore, instead of statistical methods that result in a fixed probability value, we take advantage of deep leaning to let the machine automatically learn the transition process and output the high-order dependencies. To be specific, our idea is to build a trajectory next-hop prediction model and dynamically learn a vector representation of each road. Then compute vector similarity as road correlation. Details of methods and their rationale are introduced in the following paragraphs.

\vspace{\baselineskip}

\textbf{Road Embedding.} In NLP area, how to obtain the effective representations of text words has long been a research focus. One-hot embedding is a simple solution to represent each word with a one-hot vector whose dimension is equal to the size of vocabulary. The difference between these vectors are the word indexes. However, one-hot embedding suffers from dimension curse, making the embedding vectors very large and sparse. More importantly, the vectors cannot reflect the semantic relationships between words. Therefore, word embedding technologies\cite{word_embed} are proposed to efficiently learn a fixed-length real-value vector representation for each word. And the biggest advantage is that it can catch the contextual similarity of words. If two embedding vectors are close, i.e. have a high similarity, then the two corresponding words also tend to have similar semantic meanings.

In our work, a trajectory is a sequence of different road IDs, which is similar to a sentence. Therefore, it is natural to treat each road as a word and use an embedding vector to represent it. What's more, we are inspired that road correlation can be modeled by the contextual similarity of words. To be specific, the similarity of their semantic meanings ideally models the low and high-order dependency between roads, which perfectly meets the definition of road correlation. As a result, for road $r_1$, $r_2$ and their embedding vectors $\mathbf{e_1}$ and $\mathbf{e_2}$, we define the road correlation function $Cor$ as the dot-product similarity\cite{dot_prod_simi} of the embedding vectors.
\begin{equation}
    Cor(r_1, r_2)=\mathbf{e_1}\cdot \mathbf{e_2}
\end{equation}

\subsection{Improving Traffic State Prediction}
